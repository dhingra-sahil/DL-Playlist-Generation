{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "19844b5c-1b66-47dc-eb43-f4b98438cd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse to corpus and tokenizer\n",
        "# path = '../data/'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "songstring = songstring[songstring['showdate']>'1990-01-01']\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "corpus = ' '.join(corpus).split(' ')\n",
        "\n",
        "def PrepareDataset(corpus: list, n: int,\n",
        "                   batch_size: int, train_split: float):\n",
        "    \"\"\"\n",
        "    Prepares Datasets for training and validation data from Setlist data\n",
        "    Args:\n",
        "      corpus :: list :: full corpus of songs composed of song sequences\n",
        "      n :: int :: sequence length to trim\n",
        "      batch_size :: int :: batch size for datasets\n",
        "      train_split :: float :: values between 0 and 1, splits the data for\n",
        "                              training and validation\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(corpus)-n):\n",
        "        texts.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in texts:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop last song\n",
        "        x_outputs.append(token_list[1:])  #drop first song\n",
        "\n",
        "    x_inputs = np.array(x_inputs)\n",
        "    x_outputs = np.array(x_outputs)\n",
        "\n",
        "    buffer_size = len(x_inputs)\n",
        "    train_size = int(train_split*buffer_size)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_inputs, x_outputs))\n",
        "    shuffled_data = dataset.shuffle(buffer_size)\n",
        "\n",
        "    train_data = dataset.take(train_size) \\\n",
        "                        .batch(batch_size) \\\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_data = dataset.skip(train_size) \\\n",
        "                      .batch(batch_size) \\\n",
        "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_data, val_data, tokenizer"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# classes for positional embedding and attention layers\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lscd8XJpPzyd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for NN model architecture\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.causal_self_attention(x=x)\n",
        "\n",
        "    x = self.add([x, y])\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x)\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "\n",
        "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# performance metrics\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, tokenizer= PrepareDataset(\n",
        "    corpus=corpus,\n",
        "    n=100,\n",
        "    batch_size=32,\n",
        "    train_split=0.8\n",
        ")\n",
        "\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "d_model = 128\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "num_layers = 8\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "epochs = 10\n",
        "\n",
        "gpt = GPT(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "gpt.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "gpt.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "da7b0ef4-b30d-48aa-ecd0-3cf200c60c1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "963/963 [==============================] - 162s 129ms/step - loss: 4.6570 - masked_accuracy: 0.1512 - val_loss: 4.9318 - val_masked_accuracy: 0.1712\n",
            "Epoch 2/10\n",
            "963/963 [==============================] - 115s 119ms/step - loss: 3.8084 - masked_accuracy: 0.2287 - val_loss: 5.1667 - val_masked_accuracy: 0.1693\n",
            "Epoch 3/10\n",
            "963/963 [==============================] - 116s 121ms/step - loss: 3.7265 - masked_accuracy: 0.2316 - val_loss: 5.3794 - val_masked_accuracy: 0.1608\n",
            "Epoch 4/10\n",
            "963/963 [==============================] - 115s 119ms/step - loss: 3.7474 - masked_accuracy: 0.2308 - val_loss: 5.3153 - val_masked_accuracy: 0.1670\n",
            "Epoch 5/10\n",
            "963/963 [==============================] - 123s 128ms/step - loss: 3.7354 - masked_accuracy: 0.2327 - val_loss: 5.2209 - val_masked_accuracy: 0.1756\n",
            "Epoch 6/10\n",
            "963/963 [==============================] - 115s 119ms/step - loss: 3.6876 - masked_accuracy: 0.2374 - val_loss: 5.2473 - val_masked_accuracy: 0.1671\n",
            "Epoch 7/10\n",
            "963/963 [==============================] - 123s 128ms/step - loss: 3.6360 - masked_accuracy: 0.2424 - val_loss: 5.3282 - val_masked_accuracy: 0.1591\n",
            "Epoch 8/10\n",
            "963/963 [==============================] - 114s 118ms/step - loss: 3.5952 - masked_accuracy: 0.2475 - val_loss: 5.3823 - val_masked_accuracy: 0.1628\n",
            "Epoch 9/10\n",
            "963/963 [==============================] - 114s 118ms/step - loss: 3.5625 - masked_accuracy: 0.2513 - val_loss: 5.4445 - val_masked_accuracy: 0.1578\n",
            "Epoch 10/10\n",
            "963/963 [==============================] - 131s 136ms/step - loss: 3.5354 - masked_accuracy: 0.2549 - val_loss: 5.5225 - val_masked_accuracy: 0.1592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78af501f2d10>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model the output\n",
        "test_idx = 1\n",
        "\n",
        "for input, labels in val_data.take(test_idx):\n",
        "    break\n",
        "\n",
        "test_labels = labels[0][12:-4]\n",
        "\n",
        "output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "\n",
        "for i in range(len(test_labels.numpy())):\n",
        "    output_array = output_array.write(i, test_labels.numpy()[i])\n",
        "\n",
        "\n",
        "for i in tf.range(len(input[0])):\n",
        "    output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "    predictions = gpt(output, training=False)\n",
        "\n",
        "    # Select the last token from the `seq_len` dimension.\n",
        "    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "    # Concatenate the `predicted_id` to the output which is given to the\n",
        "    # decoder as its input.\n",
        "    output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "    if predicted_id == 4:\n",
        "        break\n",
        "\n",
        "[tokenizer.index_word[s] for s in output_array.stack().numpy()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL58IFvsGN2X",
        "outputId": "27271b72-2227-45d1-de37-0eb3b8b21b9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'acdc-bag',\n",
              " 'halfway-to-the-moon',\n",
              " 'stash',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'ghost',\n",
              " 'piper',\n",
              " 'tweezer',\n",
              " 'piper',\n",
              " 'tweezer',\n",
              " 'piper',\n",
              " 'piper']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in labels[test_idx-1].numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "b1uDvSm5TIq8",
        "outputId": "5b571b3f-73d8-4fc6-e2c6-7012eb75eb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ab3d1f5f3ec6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in encoder_input.numpy()[0] if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBo5_GoGNmR",
        "outputId": "41213fa4-a019-4f20-fee9-1e7b15aee62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " 'fuego',\n",
              " 'my-soul',\n",
              " 'back-on-the-train',\n",
              " '555',\n",
              " 'dog-faced-boy',\n",
              " 'fuck-your-face',\n",
              " 'horn',\n",
              " 'frankie-says',\n",
              " 'my-friend-my-friend',\n",
              " 'roses-are-free',\n",
              " 'roggae',\n",
              " 'birds-of-a-feather',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'possum',\n",
              " 'crosseyed-and-painless',\n",
              " 'light',\n",
              " 'the-dogs',\n",
              " 'lengthwise',\n",
              " 'twist',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'harry-hood',\n",
              " 'golgi-apparatus',\n",
              " 'backwards-down-the-number-line',\n",
              " 'set-e',\n",
              " 'waiting-all-night',\n",
              " 'sing-monica',\n",
              " 'the-star-spangled-banner',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDH9j3INI488"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}