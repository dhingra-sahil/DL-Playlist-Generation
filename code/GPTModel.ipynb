{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "95e9acba-47b6-411f-b762-fae809497ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse to corpus and tokenizer\n",
        "# path = '../data/'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "songstring = songstring[songstring['showdate']>'1990-01-01']\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "corpus = ' '.join(corpus).split(' ')\n",
        "\n",
        "def PrepareDataset(corpus: list, n: int,\n",
        "                   batch_size: int, train_split: float):\n",
        "    \"\"\"\n",
        "    Prepares Datasets for training and validation data from Setlist data\n",
        "    Args:\n",
        "      corpus :: list :: full corpus of songs composed of song sequences\n",
        "      n :: int :: sequence length to trim\n",
        "      batch_size :: int :: batch size for datasets\n",
        "      train_split :: float :: values between 0 and 1, splits the data for\n",
        "                              training and validation\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(corpus)-n):\n",
        "        texts.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "    texts = texts[::n]\n",
        "\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in texts:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop last song\n",
        "        x_outputs.append(token_list[1:])  #drop first song\n",
        "\n",
        "    x_inputs = np.array(x_inputs)\n",
        "    x_outputs = np.array(x_outputs)\n",
        "\n",
        "    buffer_size = len(x_inputs)\n",
        "    train_size = int(train_split*buffer_size)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_inputs, x_outputs))\n",
        "    shuffled_data = dataset.shuffle(buffer_size)\n",
        "\n",
        "    train_data = dataset.take(train_size) \\\n",
        "                        .batch(batch_size) \\\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_data = dataset.skip(train_size) \\\n",
        "                      .batch(batch_size) \\\n",
        "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_data, val_data, tokenizer"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# classes for positional embedding and attention layers\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lscd8XJpPzyd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for NN model architecture\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.causal_self_attention(x=x)\n",
        "\n",
        "    x = self.add([x, y])\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x)\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, x):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "\n",
        "    x = self.decoder(x)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# performance metrics\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, tokenizer= PrepareDataset(\n",
        "    corpus=corpus,\n",
        "    n=125,\n",
        "    batch_size=32,\n",
        "    train_split=0.8\n",
        ")\n",
        "\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "d_model = 128\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "num_layers = 8\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "epochs = 100\n",
        "\n",
        "gpt = GPT(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "gpt.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "gpt.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "a9eaaea0-0827-4137-8ccf-c79b2fcf1ef7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 31s 631ms/step - loss: 6.2958 - masked_accuracy: 0.0033 - val_loss: 6.2913 - val_masked_accuracy: 0.0037\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 6.2767 - masked_accuracy: 0.0027 - val_loss: 6.2597 - val_masked_accuracy: 0.0036\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 6.2291 - masked_accuracy: 0.0048 - val_loss: 6.2086 - val_masked_accuracy: 0.0036\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 6.1680 - masked_accuracy: 0.0080 - val_loss: 6.1441 - val_masked_accuracy: 0.0079\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 6.1010 - masked_accuracy: 0.0141 - val_loss: 6.0748 - val_masked_accuracy: 0.0203\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 6.0252 - masked_accuracy: 0.0251 - val_loss: 6.0089 - val_masked_accuracy: 0.0341\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 5.9535 - masked_accuracy: 0.0339 - val_loss: 5.9511 - val_masked_accuracy: 0.0448\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 5.8882 - masked_accuracy: 0.0422 - val_loss: 5.9019 - val_masked_accuracy: 0.0518\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 5.8298 - masked_accuracy: 0.0499 - val_loss: 5.8595 - val_masked_accuracy: 0.0612\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 5.7732 - masked_accuracy: 0.0576 - val_loss: 5.8230 - val_masked_accuracy: 0.0708\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 5.7268 - masked_accuracy: 0.0654 - val_loss: 5.7905 - val_masked_accuracy: 0.0758\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 5.6876 - masked_accuracy: 0.0697 - val_loss: 5.7606 - val_masked_accuracy: 0.0777\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 5.6488 - masked_accuracy: 0.0727 - val_loss: 5.7327 - val_masked_accuracy: 0.0787\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 5.6083 - masked_accuracy: 0.0745 - val_loss: 5.7055 - val_masked_accuracy: 0.0792\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 5.5702 - masked_accuracy: 0.0755 - val_loss: 5.6761 - val_masked_accuracy: 0.0815\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 5.5291 - masked_accuracy: 0.0777 - val_loss: 5.6421 - val_masked_accuracy: 0.0828\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 5.4855 - masked_accuracy: 0.0792 - val_loss: 5.6059 - val_masked_accuracy: 0.0842\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 5.4376 - masked_accuracy: 0.0787 - val_loss: 5.5726 - val_masked_accuracy: 0.0855\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 5.3895 - masked_accuracy: 0.0808 - val_loss: 5.5441 - val_masked_accuracy: 0.0862\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 5.3462 - masked_accuracy: 0.0823 - val_loss: 5.5228 - val_masked_accuracy: 0.0876\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 5.3051 - masked_accuracy: 0.0835 - val_loss: 5.5060 - val_masked_accuracy: 0.0870\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 5.2673 - masked_accuracy: 0.0840 - val_loss: 5.4912 - val_masked_accuracy: 0.0882\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 5.2313 - masked_accuracy: 0.0866 - val_loss: 5.4777 - val_masked_accuracy: 0.0881\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 5.1978 - masked_accuracy: 0.0867 - val_loss: 5.4661 - val_masked_accuracy: 0.0898\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 5.1640 - masked_accuracy: 0.0884 - val_loss: 5.4546 - val_masked_accuracy: 0.0917\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 5.1289 - masked_accuracy: 0.0922 - val_loss: 5.4427 - val_masked_accuracy: 0.0927\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 5.0914 - masked_accuracy: 0.0962 - val_loss: 5.4306 - val_masked_accuracy: 0.0950\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 5.0516 - masked_accuracy: 0.1003 - val_loss: 5.4174 - val_masked_accuracy: 0.0974\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 5.0155 - masked_accuracy: 0.1058 - val_loss: 5.4056 - val_masked_accuracy: 0.0986\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 4.9767 - masked_accuracy: 0.1090 - val_loss: 5.3933 - val_masked_accuracy: 0.1009\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 4.9340 - masked_accuracy: 0.1165 - val_loss: 5.3792 - val_masked_accuracy: 0.1063\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 4.8949 - masked_accuracy: 0.1202 - val_loss: 5.3693 - val_masked_accuracy: 0.1080\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 4.8605 - masked_accuracy: 0.1257 - val_loss: 5.3601 - val_masked_accuracy: 0.1081\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 4.8234 - masked_accuracy: 0.1273 - val_loss: 5.3547 - val_masked_accuracy: 0.1090\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 4.7879 - masked_accuracy: 0.1328 - val_loss: 5.3476 - val_masked_accuracy: 0.1104\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 1s 141ms/step - loss: 4.7546 - masked_accuracy: 0.1361 - val_loss: 5.3423 - val_masked_accuracy: 0.1133\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 1s 156ms/step - loss: 4.7273 - masked_accuracy: 0.1396 - val_loss: 5.3361 - val_masked_accuracy: 0.1149\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 4.6993 - masked_accuracy: 0.1419 - val_loss: 5.3344 - val_masked_accuracy: 0.1160\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 4.6753 - masked_accuracy: 0.1448 - val_loss: 5.3404 - val_masked_accuracy: 0.1149\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 1s 158ms/step - loss: 4.6430 - masked_accuracy: 0.1473 - val_loss: 5.3437 - val_masked_accuracy: 0.1139\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 1s 149ms/step - loss: 4.6130 - masked_accuracy: 0.1534 - val_loss: 5.3385 - val_masked_accuracy: 0.1164\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 4.5881 - masked_accuracy: 0.1543 - val_loss: 5.3314 - val_masked_accuracy: 0.1192\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 4.5639 - masked_accuracy: 0.1573 - val_loss: 5.3223 - val_masked_accuracy: 0.1236\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 1s 173ms/step - loss: 4.5391 - masked_accuracy: 0.1616 - val_loss: 5.3134 - val_masked_accuracy: 0.1251\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 1s 164ms/step - loss: 4.5130 - masked_accuracy: 0.1653 - val_loss: 5.3151 - val_masked_accuracy: 0.1264\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 4.4877 - masked_accuracy: 0.1688 - val_loss: 5.3173 - val_masked_accuracy: 0.1250\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 4.4634 - masked_accuracy: 0.1709 - val_loss: 5.3178 - val_masked_accuracy: 0.1259\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 4.4337 - masked_accuracy: 0.1733 - val_loss: 5.3073 - val_masked_accuracy: 0.1286\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 4.4141 - masked_accuracy: 0.1756 - val_loss: 5.2938 - val_masked_accuracy: 0.1313\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 1s 147ms/step - loss: 4.3929 - masked_accuracy: 0.1802 - val_loss: 5.2605 - val_masked_accuracy: 0.1388\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 4.3865 - masked_accuracy: 0.1798 - val_loss: 5.2512 - val_masked_accuracy: 0.1410\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 4.3588 - masked_accuracy: 0.1803 - val_loss: 5.2577 - val_masked_accuracy: 0.1364\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 4.3339 - masked_accuracy: 0.1875 - val_loss: 5.2774 - val_masked_accuracy: 0.1358\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 1s 143ms/step - loss: 4.2968 - masked_accuracy: 0.1922 - val_loss: 5.2342 - val_masked_accuracy: 0.1460\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 4.2778 - masked_accuracy: 0.1930 - val_loss: 5.2039 - val_masked_accuracy: 0.1505\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 4.2535 - masked_accuracy: 0.1978 - val_loss: 5.1833 - val_masked_accuracy: 0.1459\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 1s 137ms/step - loss: 4.2356 - masked_accuracy: 0.2020 - val_loss: 5.1412 - val_masked_accuracy: 0.1580\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 4.2162 - masked_accuracy: 0.2035 - val_loss: 5.1423 - val_masked_accuracy: 0.1547\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 1s 136ms/step - loss: 4.1881 - masked_accuracy: 0.2052 - val_loss: 5.1501 - val_masked_accuracy: 0.1590\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 4.1822 - masked_accuracy: 0.2078 - val_loss: 5.1841 - val_masked_accuracy: 0.1643\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 4.1352 - masked_accuracy: 0.2131 - val_loss: 5.2261 - val_masked_accuracy: 0.1550\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 4.1235 - masked_accuracy: 0.2099 - val_loss: 5.1258 - val_masked_accuracy: 0.1574\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 4.1297 - masked_accuracy: 0.2138 - val_loss: 5.1621 - val_masked_accuracy: 0.1619\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 4.0793 - masked_accuracy: 0.2158 - val_loss: 5.1417 - val_masked_accuracy: 0.1680\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 4.0464 - masked_accuracy: 0.2199 - val_loss: 5.1355 - val_masked_accuracy: 0.1559\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 4.0600 - masked_accuracy: 0.2167 - val_loss: 5.1101 - val_masked_accuracy: 0.1628\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 1s 155ms/step - loss: 4.0089 - masked_accuracy: 0.2261 - val_loss: 5.0535 - val_masked_accuracy: 0.1719\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 3.9856 - masked_accuracy: 0.2286 - val_loss: 5.0544 - val_masked_accuracy: 0.1744\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.9576 - masked_accuracy: 0.2303 - val_loss: 5.0462 - val_masked_accuracy: 0.1716\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.9402 - masked_accuracy: 0.2319 - val_loss: 5.0193 - val_masked_accuracy: 0.1766\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.9298 - masked_accuracy: 0.2347 - val_loss: 5.0497 - val_masked_accuracy: 0.1784\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.9101 - masked_accuracy: 0.2356 - val_loss: 5.1418 - val_masked_accuracy: 0.1695\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8819 - masked_accuracy: 0.2365 - val_loss: 5.0389 - val_masked_accuracy: 0.1807\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.9408 - masked_accuracy: 0.2321 - val_loss: 5.0513 - val_masked_accuracy: 0.1737\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8610 - masked_accuracy: 0.2401 - val_loss: 5.0357 - val_masked_accuracy: 0.1730\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.8435 - masked_accuracy: 0.2385 - val_loss: 5.0429 - val_masked_accuracy: 0.1795\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 1s 151ms/step - loss: 3.8150 - masked_accuracy: 0.2431 - val_loss: 5.0439 - val_masked_accuracy: 0.1816\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 3.8109 - masked_accuracy: 0.2439 - val_loss: 5.0043 - val_masked_accuracy: 0.1771\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 1s 150ms/step - loss: 3.8036 - masked_accuracy: 0.2441 - val_loss: 4.9911 - val_masked_accuracy: 0.1798\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 3.7744 - masked_accuracy: 0.2460 - val_loss: 5.0617 - val_masked_accuracy: 0.1792\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 1s 152ms/step - loss: 3.7716 - masked_accuracy: 0.2443 - val_loss: 5.1221 - val_masked_accuracy: 0.1858\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.7339 - masked_accuracy: 0.2461 - val_loss: 5.0503 - val_masked_accuracy: 0.1800\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.7662 - masked_accuracy: 0.2467 - val_loss: 5.0205 - val_masked_accuracy: 0.1810\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.7091 - masked_accuracy: 0.2504 - val_loss: 5.0824 - val_masked_accuracy: 0.1845\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 1s 144ms/step - loss: 3.6914 - masked_accuracy: 0.2516 - val_loss: 5.0659 - val_masked_accuracy: 0.1856\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.7081 - masked_accuracy: 0.2553 - val_loss: 5.0266 - val_masked_accuracy: 0.1800\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.6933 - masked_accuracy: 0.2559 - val_loss: 5.0958 - val_masked_accuracy: 0.1856\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.6746 - masked_accuracy: 0.2534 - val_loss: 5.1146 - val_masked_accuracy: 0.1852\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 1s 138ms/step - loss: 3.6626 - masked_accuracy: 0.2558 - val_loss: 5.0593 - val_masked_accuracy: 0.1789\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.7139 - masked_accuracy: 0.2522 - val_loss: 5.1026 - val_masked_accuracy: 0.1838\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 1s 154ms/step - loss: 3.6294 - masked_accuracy: 0.2568 - val_loss: 5.1148 - val_masked_accuracy: 0.1877\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 1s 153ms/step - loss: 3.6038 - masked_accuracy: 0.2575 - val_loss: 5.0972 - val_masked_accuracy: 0.1800\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 3.6051 - masked_accuracy: 0.2618 - val_loss: 5.1351 - val_masked_accuracy: 0.1877\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 1s 157ms/step - loss: 3.5868 - masked_accuracy: 0.2613 - val_loss: 5.0684 - val_masked_accuracy: 0.1861\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 3.5622 - masked_accuracy: 0.2651 - val_loss: 5.1322 - val_masked_accuracy: 0.1824\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 1s 142ms/step - loss: 3.5490 - masked_accuracy: 0.2642 - val_loss: 5.1081 - val_masked_accuracy: 0.1852\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.5445 - masked_accuracy: 0.2665 - val_loss: 5.1148 - val_masked_accuracy: 0.1867\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.5106 - masked_accuracy: 0.2674 - val_loss: 5.1010 - val_masked_accuracy: 0.1839\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 1s 139ms/step - loss: 3.5375 - masked_accuracy: 0.2669 - val_loss: 5.1171 - val_masked_accuracy: 0.1853\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 1s 140ms/step - loss: 3.4909 - masked_accuracy: 0.2722 - val_loss: 5.1110 - val_masked_accuracy: 0.1860\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7992fe1bbf40>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model the output\n",
        "test_idx = 1\n",
        "\n",
        "for input, labels in val_data.take(test_idx):\n",
        "    break\n",
        "\n",
        "test_labels = labels[0][12:-13]\n",
        "\n",
        "output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "\n",
        "for i in range(len(test_labels.numpy())):\n",
        "    output_array = output_array.write(i, test_labels.numpy()[i])\n",
        "\n",
        "# output_array = output_array.write(0, 1)\n",
        "# output_array = output_array.write(1, 100)\n",
        "\n",
        "for i in tf.range(len(output_array.stack().numpy()), len(input[0])*2):\n",
        "    output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "    predictions = gpt(output, training=False)\n",
        "\n",
        "    # Select the last token from the `seq_len` dimension.\n",
        "    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "    # Concatenate the `predicted_id` to the output which is given to the\n",
        "    # decoder as its input.\n",
        "    output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "    if predicted_id == 4:\n",
        "        break\n",
        "\n",
        "[tokenizer.index_word[s] for s in output_array.stack().numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL58IFvsGN2X",
        "outputId": "db18f8c6-991d-4b77-b9d9-b6ec7a236713"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['funky-bitch',\n",
              " 'maze',\n",
              " 'ocelot',\n",
              " 'sparkle',\n",
              " 'cavern',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'carini',\n",
              " 'ghost',\n",
              " 'mikes-song',\n",
              " 'simple',\n",
              " 'joy',\n",
              " 'weekapaug-groove',\n",
              " 'julius',\n",
              " 'sand',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e',\n",
              " 'quinn-the-eskimo-the-mighty-quinn',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'my-soul',\n",
              " 'bathtub-gin',\n",
              " '555',\n",
              " 'pebbles-and-marbles',\n",
              " 'the-line',\n",
              " 'vultures',\n",
              " 'fast-enough-for-you',\n",
              " 'back-on-the-train',\n",
              " 'taste',\n",
              " 'gumbo',\n",
              " 'halfway-to-the-moon',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " 'chalk-dust-torture',\n",
              " 'scents-and-subtle-sounds',\n",
              " 'twist',\n",
              " 'fuego',\n",
              " 'the-wedge',\n",
              " 'light',\n",
              " 'harry-hood',\n",
              " 'first-tube',\n",
              " 'set-e',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'llama',\n",
              " 'undermind',\n",
              " 'stash',\n",
              " 'halfway-to-the-moon',\n",
              " 'i-didnt-know',\n",
              " 'nellie-kane',\n",
              " 'guyute',\n",
              " 'the-line',\n",
              " 'ocelot',\n",
              " 'no-quarter',\n",
              " 'ha-ha-ha',\n",
              " 'suzy-greenberg',\n",
              " 'set-2',\n",
              " '46-days',\n",
              " 'back-on-the-train',\n",
              " 'simple',\n",
              " 'ghost',\n",
              " 'backwards-down-the-number-line',\n",
              " 'harry-hood',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'run-like-an-antelope',\n",
              " 'set-e',\n",
              " 'character-zero',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'free',\n",
              " 'the-moma-dance',\n",
              " 'halleys-comet',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " '555',\n",
              " 'rift',\n",
              " 'sample-in-a-jar',\n",
              " 'devotion-to-a-dream',\n",
              " 'yarmouth-road',\n",
              " 'sparkle',\n",
              " 'wingsuit',\n",
              " 'david-bowie',\n",
              " 'cavern',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'whats-the-use',\n",
              " 'carini',\n",
              " 'light',\n",
              " 'fuego',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'meatstick',\n",
              " 'bold-as-love',\n",
              " 'set-e',\n",
              " 'the-horse',\n",
              " 'silent-in-the-morning',\n",
              " 'fluffhead',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'kill-devil-falls',\n",
              " 'the-moma-dance',\n",
              " 'rift',\n",
              " 'bathtub-gin',\n",
              " 'the-moma-dance',\n",
              " 'rift',\n",
              " 'free',\n",
              " 'ocelot',\n",
              " 'stash',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'sand',\n",
              " 'twist',\n",
              " 'piper',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'piper',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'harry-hood',\n",
              " 'set-e',\n",
              " 'loving-cup',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels)\n",
        "\n",
        "print(output_array.stack())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkzGors9Ydh6",
        "outputId": "9feadbca-3ffd-4644-f807-068c70c2a631"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 62  25 110  27  11 197   2  85  49   9  51 167  10  42  80 108   8   3\n",
            " 228   4   1 126  35 152 238 209 186 134  79  83  94 182 133  19   2   7\n",
            " 198  70 131  89  93  21  92   3  52   4   1  30 170  16 182  54 136  87\n",
            " 209 110 267 219  19   2  84  79  51  49  88  21 108  15   3  41   4   1\n",
            "  48  56  82 133 152  33  31 261 223  27 197  17  11   2  28 158  85  93\n",
            " 131  44 143 165   3  66  60  52   4], shape=(99,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 62  25 110  27  11 197   2  85  49   9  51 167  10  42  80 108   8   3\n",
            " 228   4   1 126  35 152 238 209 186 134  79  83  94 182 133  19   2   7\n",
            " 198  70 131  89  93  21  92   3  52   4   1  30 170  16 182  54 136  87\n",
            " 209 110 267 219  19   2  84  79  51  49  88  21 108  15   3  41   4   1\n",
            "  48  56  82 133 152  33  31 261 223  27 197  17  11   2  28 158  85  93\n",
            " 131  44 143 165   3  66  60  52   4   0   1 103  56  33  35  56  33  48\n",
            " 110  16   2  28  80  70  57 108  57 108  21   3  77   4], shape=(122,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in labels[test_idx-1].numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "b1uDvSm5TIq8",
        "outputId": "5b571b3f-73d8-4fc6-e2c6-7012eb75eb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ab3d1f5f3ec6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in encoder_input.numpy()[0] if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBo5_GoGNmR",
        "outputId": "41213fa4-a019-4f20-fee9-1e7b15aee62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " 'fuego',\n",
              " 'my-soul',\n",
              " 'back-on-the-train',\n",
              " '555',\n",
              " 'dog-faced-boy',\n",
              " 'fuck-your-face',\n",
              " 'horn',\n",
              " 'frankie-says',\n",
              " 'my-friend-my-friend',\n",
              " 'roses-are-free',\n",
              " 'roggae',\n",
              " 'birds-of-a-feather',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'possum',\n",
              " 'crosseyed-and-painless',\n",
              " 'light',\n",
              " 'the-dogs',\n",
              " 'lengthwise',\n",
              " 'twist',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'harry-hood',\n",
              " 'golgi-apparatus',\n",
              " 'backwards-down-the-number-line',\n",
              " 'set-e',\n",
              " 'waiting-all-night',\n",
              " 'sing-monica',\n",
              " 'the-star-spangled-banner',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDH9j3INI488"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}