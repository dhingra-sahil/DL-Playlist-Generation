{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "bb42f111-74c4-4f2a-c3c5-a0114fc72889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse to corpus and tokenizer\n",
        "# path = '../data/'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "def PrepareDataset(corpus: list, tokenizer: Tokenizer,\n",
        "                   n_context: int, batch_size: int, train_split: float):\n",
        "    \"\"\"\n",
        "    Prepares Datasets for training and validation data from Setlist data\n",
        "    Args:\n",
        "      corpus :: list :: full corpus of songs composed of setlists as sequences\n",
        "      tokenizer :: Tokenizer :: keras Tokenizer object trained on corpus\n",
        "      n_context :: int :: number of previous setlist to use as context for a\n",
        "                          given setlist\n",
        "      batch_size :: int :: batch size for datasets\n",
        "      train_split :: float :: values between 0 and 1, splits the data for\n",
        "                              training and validation\n",
        "    \"\"\"\n",
        "    max_seq_length = max([len(setlist.split(' ')) for setlist in corpus]) - 1\n",
        "\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop eos\n",
        "        x_outputs.append(token_list[1:])  #drop set-1\n",
        "\n",
        "    x_inputs = np.array(\n",
        "        pad_sequences(x_inputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "    x_outputs = np.array(\n",
        "        pad_sequences(x_outputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "\n",
        "    # last n shows as the context vector for each show\n",
        "    n = n_context\n",
        "    n_shows = []\n",
        "    for i in range(len(corpus[:-n])):\n",
        "        n_shows.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "    context = []\n",
        "    for line in n_shows:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        context.append(token_list)\n",
        "\n",
        "    max_context_length = max([len(x) for x in context])\n",
        "    x_context = np.array(\n",
        "        pad_sequences(context, maxlen=max_context_length, padding='post')\n",
        "    )\n",
        "\n",
        "    x_context = x_context[:-1]\n",
        "    x_inputs = x_inputs[n+1:]\n",
        "    x_outputs = x_outputs[n+1:]\n",
        "\n",
        "    buffer_size = len(x_context)\n",
        "    train_size = int(train_split*buffer_size)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((x_context, x_inputs), x_outputs))\n",
        "    shuffled_data = dataset.shuffle(buffer_size)\n",
        "\n",
        "    train_data = dataset.take(train_size) \\\n",
        "                        .batch(batch_size) \\\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    val_data = dataset.skip(train_size) \\\n",
        "                      .batch(batch_size) \\\n",
        "                      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_data, val_data"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# classes for positional embedding and attention layers\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lscd8XJpPzyd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes for NN model architecture\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "# performance metrics\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = PrepareDataset(\n",
        "    corpus=corpus,\n",
        "    tokenizer=tokenizer,\n",
        "    n_context=5,\n",
        "    batch_size=8,\n",
        "    train_split=0.8\n",
        ")\n",
        "\n",
        "d_model = 256\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "num_layers = 4\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.15\n",
        "epochs = 10\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "transformer.fit(train_data, validation_data=val_data, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "bcc7057c-7e6c-455d-934b-047469458e50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "155/155 [==============================] - 62s 207ms/step - loss: 5.7347 - masked_accuracy: 0.0548 - val_loss: 5.5127 - val_masked_accuracy: 0.1070\n",
            "Epoch 2/10\n",
            "155/155 [==============================] - 18s 114ms/step - loss: 5.0110 - masked_accuracy: 0.1055 - val_loss: 5.3010 - val_masked_accuracy: 0.1322\n",
            "Epoch 3/10\n",
            "155/155 [==============================] - 18s 113ms/step - loss: 4.5676 - masked_accuracy: 0.1540 - val_loss: 5.1650 - val_masked_accuracy: 0.1548\n",
            "Epoch 4/10\n",
            "155/155 [==============================] - 19s 125ms/step - loss: 4.2576 - masked_accuracy: 0.1831 - val_loss: 5.1728 - val_masked_accuracy: 0.1573\n",
            "Epoch 5/10\n",
            "155/155 [==============================] - 19s 120ms/step - loss: 4.0527 - masked_accuracy: 0.2019 - val_loss: 5.1726 - val_masked_accuracy: 0.1558\n",
            "Epoch 6/10\n",
            "155/155 [==============================] - 17s 113ms/step - loss: 3.9023 - masked_accuracy: 0.2141 - val_loss: 5.2778 - val_masked_accuracy: 0.1540\n",
            "Epoch 7/10\n",
            "155/155 [==============================] - 18s 113ms/step - loss: 3.7870 - masked_accuracy: 0.2224 - val_loss: 5.3412 - val_masked_accuracy: 0.1579\n",
            "Epoch 8/10\n",
            "155/155 [==============================] - 18s 113ms/step - loss: 3.6978 - masked_accuracy: 0.2294 - val_loss: 5.4417 - val_masked_accuracy: 0.1615\n",
            "Epoch 9/10\n",
            "155/155 [==============================] - 17s 113ms/step - loss: 3.6247 - masked_accuracy: 0.2365 - val_loss: 5.5458 - val_masked_accuracy: 0.1595\n",
            "Epoch 10/10\n",
            "155/155 [==============================] - 19s 121ms/step - loss: 3.5441 - masked_accuracy: 0.2443 - val_loss: 5.7308 - val_masked_accuracy: 0.1588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b41f1b8a050>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model the output\n",
        "\n",
        "for (context, input), labels in val_data.take(3):\n",
        "  break\n",
        "\n",
        "encoder_input = tf.reshape(context[0], (1, len(context[0])))\n",
        "\n",
        "output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "output_array = output_array.write(0, 1)\n",
        "\n",
        "for i in tf.range(25):\n",
        "    output = tf.reshape(output_array.stack(), (1, len(output_array.stack())))\n",
        "    predictions = transformer([encoder_input, output], training=False)\n",
        "\n",
        "    # Select the last token from the `seq_len` dimension.\n",
        "    predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "    predicted_id = tf.argmax(predictions, axis=-1)[0][0]\n",
        "\n",
        "    # Concatenate the `predicted_id` to the output which is given to the\n",
        "    # decoder as its input.\n",
        "    output_array = output_array.write(i+1, predicted_id)\n",
        "\n",
        "    if predicted_id == 4:\n",
        "        break\n",
        "\n",
        "[tokenizer.index_word[s] for s in output_array.stack().numpy()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL58IFvsGN2X",
        "outputId": "64b4dc2e-efe7-4452-bd69-21de4f1de57e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " 'the-moma-dance',\n",
              " 'rift',\n",
              " 'wolfmans-brother',\n",
              " 'scent-of-a-mule',\n",
              " 'sample-in-a-jar',\n",
              " 'divided-sky',\n",
              " 'lawn-boy',\n",
              " '46-days',\n",
              " 'set-2',\n",
              " 'down-with-disease',\n",
              " 'free',\n",
              " 'light',\n",
              " 'tweezer',\n",
              " 'piper',\n",
              " 'twist',\n",
              " 'piper',\n",
              " 'twist',\n",
              " 'piper',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'set-e',\n",
              " 'loving-cup',\n",
              " 'tweezer-reprise',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in labels[0].numpy() if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1uDvSm5TIq8",
        "outputId": "e7c064ec-2fc7-4ce4-e37e-bd53392676e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fuego',\n",
              " 'my-soul',\n",
              " 'back-on-the-train',\n",
              " '555',\n",
              " 'dog-faced-boy',\n",
              " 'fuck-your-face',\n",
              " 'horn',\n",
              " 'frankie-says',\n",
              " 'my-friend-my-friend',\n",
              " 'roses-are-free',\n",
              " 'roggae',\n",
              " 'birds-of-a-feather',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'possum',\n",
              " 'crosseyed-and-painless',\n",
              " 'light',\n",
              " 'the-dogs',\n",
              " 'lengthwise',\n",
              " 'twist',\n",
              " 'wading-in-the-velvet-sea',\n",
              " 'harry-hood',\n",
              " 'golgi-apparatus',\n",
              " 'backwards-down-the-number-line',\n",
              " 'set-e',\n",
              " 'waiting-all-night',\n",
              " 'sing-monica',\n",
              " 'the-star-spangled-banner',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[s] for s in encoder_input.numpy()[0] if s != 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBo5_GoGNmR",
        "outputId": "437026e7-8d36-4d29-f2db-6b5440a7c557"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set-1',\n",
              " '46-days',\n",
              " 'tube',\n",
              " 'train-song',\n",
              " 'ghost',\n",
              " 'sparkle',\n",
              " 'sample-in-a-jar',\n",
              " 'divided-sky',\n",
              " 'the-line',\n",
              " 'its-ice',\n",
              " 'kill-devil-falls',\n",
              " 'bathtub-gin',\n",
              " 'set-2',\n",
              " '555',\n",
              " 'backwards-down-the-number-line',\n",
              " 'down-with-disease',\n",
              " 'fuego',\n",
              " 'twist',\n",
              " 'bouncing-around-the-room',\n",
              " 'david-bowie',\n",
              " 'character-zero',\n",
              " 'set-e',\n",
              " 'harry-hood',\n",
              " 'grind',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'devotion-to-a-dream',\n",
              " 'acdc-bag',\n",
              " 'my-sweet-one',\n",
              " 'the-moma-dance',\n",
              " 'halleys-comet',\n",
              " 'funky-bitch',\n",
              " 'wolfmans-brother',\n",
              " 'destiny-unbound',\n",
              " 'timber-jerry-the-mule',\n",
              " 'tela',\n",
              " 'wingsuit',\n",
              " 'set-2',\n",
              " 'free',\n",
              " 'golden-age',\n",
              " 'gotta-jibboo',\n",
              " 'carini',\n",
              " 'piper',\n",
              " 'prince-caspian',\n",
              " 'tweezer',\n",
              " 'rock-and-roll',\n",
              " 'you-enjoy-myself',\n",
              " 'set-e',\n",
              " 'suzy-greenberg',\n",
              " 'tweezer-reprise',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'walfredo',\n",
              " 'ocelot',\n",
              " 'camel-walk',\n",
              " 'axilla',\n",
              " 'rift',\n",
              " '555',\n",
              " 'maze',\n",
              " 'brian-and-robert',\n",
              " 'stash',\n",
              " 'party-time',\n",
              " '46-days',\n",
              " 'set-2',\n",
              " 'sand',\n",
              " 'birds-of-a-feather',\n",
              " 'waiting-all-night',\n",
              " 'ghost',\n",
              " 'bug',\n",
              " 'seven-below',\n",
              " 'i-didnt-know',\n",
              " 'chalk-dust-torture',\n",
              " 'also-sprach-zarathustra',\n",
              " 'slave-to-the-traffic-light',\n",
              " 'set-e',\n",
              " 'wildcard',\n",
              " 'winterqueen',\n",
              " 'a-day-in-the-life',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'crowd-control',\n",
              " 'mikes-song',\n",
              " 'i-am-hydrogen',\n",
              " 'weekapaug-groove',\n",
              " 'wingsuit',\n",
              " 'water-in-the-sky',\n",
              " 'plasma',\n",
              " 'halfway-to-the-moon',\n",
              " 'poor-heart',\n",
              " 'gumbo',\n",
              " 'sanity',\n",
              " 'run-like-an-antelope',\n",
              " 'set-2',\n",
              " 'kill-devil-falls',\n",
              " 'mountains-in-the-mist',\n",
              " 'fuego',\n",
              " 'julius',\n",
              " 'twist',\n",
              " 'runaway-jim',\n",
              " 'harry-hood',\n",
              " 'set-e',\n",
              " 'loving-cup',\n",
              " 'eos',\n",
              " 'set-1',\n",
              " 'stealing-time-from-the-faulty-plan',\n",
              " 'the-moma-dance',\n",
              " 'free',\n",
              " 'back-on-the-train',\n",
              " 'yarmouth-road',\n",
              " 'strange-design',\n",
              " 'taste',\n",
              " 'the-wedge',\n",
              " 'the-line',\n",
              " 'wolfmans-brother',\n",
              " 'set-2',\n",
              " 'first-tube',\n",
              " 'down-with-disease',\n",
              " 'theme-from-the-bottom',\n",
              " 'split-open-and-melt',\n",
              " 'heavy-things',\n",
              " 'light',\n",
              " 'possum',\n",
              " 'set-e',\n",
              " 'contact',\n",
              " 'meatstick',\n",
              " 'character-zero',\n",
              " 'eos']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDH9j3INI488"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}