{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSO58G6FGC19",
        "outputId": "120d9a70-f818-4fb8-d82f-9e5583b25e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# read-in cleaned data and parse.\n",
        "# path = '../data/'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "filepath = path + 'allphishsets.csv'\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "df = df.sort_values(by=['showdate', 'set', 'position'],\n",
        "                    ascending=[True, True, True])\n",
        "\n",
        "df.loc[df['times_played'] <= 2, 'slug'] = 'wildcard'\n",
        "df.loc[df['times_played'] <= 2, 'times_played'] = 510\n",
        "\n",
        "songstring = df[['showdate', 'set', 'slug']].groupby(['showdate', 'set'])['slug']\\\n",
        "                                            .apply(lambda x: '|'.join(x)).reset_index()\n",
        "songstring['full'] = songstring.apply(lambda row: f\"set-{row['set']}|{row['slug']}\", axis=1)\n",
        "\n",
        "songstring = songstring[['showdate', 'full']].groupby(['showdate'])['full']\\\n",
        "                                             .apply(lambda x: '|'.join(x)).reset_index()\n",
        "\n",
        "songstring['full'] += '|eos'\n",
        "\n",
        "corpus = [''.join(map(lambda s: s.replace('|', ' '), f))\n",
        "          for f in songstring['full']]\n",
        "\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "unique_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_seq_length = max([len(setlist.split(' ')) for setlist in corpus]) - 1\n",
        "\n",
        "def PrepSequences(corpus: list, tokenizer: Tokenizer, max_seq_length=None):\n",
        "    x_inputs = []\n",
        "    x_outputs = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        x_inputs.append(token_list[:-1])  #drop eos\n",
        "        x_outputs.append(token_list[1:])  #drop set-1\n",
        "    if max_seq_length is None:\n",
        "        max_seq_length = max([len(x) for x in x_inputs])\n",
        "    x_inputs = np.array(\n",
        "        pad_sequences(x_inputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "    x_outputs = np.array(\n",
        "        pad_sequences(x_outputs, maxlen=max_seq_length, padding='post')\n",
        "    )\n",
        "    return x_inputs, x_outputs\n",
        "\n",
        "x_inputs, x_outputs = PrepSequences(\n",
        "    corpus=corpus, tokenizer=tokenizer, max_seq_length=max_seq_length\n",
        ")\n",
        "\n",
        "# last n shows as the context for each show\n",
        "n = 5\n",
        "n_shows = []\n",
        "for i in range(len(corpus[:-n])):\n",
        "    n_shows.append(' '.join(corpus[i:i+n]))\n",
        "\n",
        "context = []\n",
        "for line in n_shows:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    context.append(token_list)\n",
        "\n",
        "max_context_length = max([len(x) for x in context])\n",
        "x_context = np.array(\n",
        "    pad_sequences(context, maxlen=max_context_length, padding='post')\n",
        ")\n",
        "\n",
        "x_context = x_context[:-1]\n",
        "x_inputs = x_inputs[n+1:]\n",
        "x_outputs = x_outputs[n+1:]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((x_context, x_inputs), x_outputs))\n",
        "\n",
        "train_batches = dataset.shuffle(len(x_context)) \\\n",
        "                       .batch(64) \\\n",
        "                       .prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qoaZAlnwP9qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n",
        "\n",
        "\n",
        "embed_pos = PositionalEmbedding(vocab_size=unique_words, d_model=512)\n",
        "xin_emb = embed_pos(x_inputs)\n",
        "xcon_emb = embed_pos(x_context)"
      ],
      "metadata": {
        "id": "l_s_B8SoR3Lp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(xin_emb.shape)\n",
        "print(sample_ca(xin_emb, xcon_emb).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YhNWmg8PxtL",
        "outputId": "230e5090-2ed4-4ed4-8e40-f9794d74dabd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1544, 55, 512)\n",
            "(1544, 55, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(xcon_emb.shape)\n",
        "print(sample_gsa(xcon_emb).shape)\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(xin_emb.shape)\n",
        "print(sample_csa(xin_emb).shape)\n",
        "\n",
        "out1 = sample_csa(embed_pos(x_inputs[:, :3]))\n",
        "out2 = sample_csa(embed_pos(x_inputs))[:, :3]\n",
        "\n",
        "print(tf.reduce_max(abs(out1 - out2)).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lscd8XJpPzyd",
        "outputId": "520189f6-9054-44ce-a639-b24dcf610d3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1544, 170, 512)\n",
            "(1544, 170, 512)\n",
            "(1544, 55, 512)\n",
            "(1544, 55, 512)\n",
            "4.7683716e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zHehKuMBGOUJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=unique_words,\n",
        "    target_vocab_size=unique_words,\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])\n",
        "\n",
        "transformer.fit(train_batches, epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJYGwRyGOLm",
        "outputId": "473185dc-6b67-485d-cda1-06202dc7639b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 44s 358ms/step - loss: 6.3499 - masked_accuracy: 0.0048\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 8s 298ms/step - loss: 6.1202 - masked_accuracy: 0.0250\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 8s 302ms/step - loss: 5.8906 - masked_accuracy: 0.0458\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 7s 293ms/step - loss: 5.7555 - masked_accuracy: 0.0568\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 7s 290ms/step - loss: 5.6525 - masked_accuracy: 0.0708\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 9s 345ms/step - loss: 5.5338 - masked_accuracy: 0.0841\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 7s 288ms/step - loss: 5.4028 - masked_accuracy: 0.0932\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 7s 292ms/step - loss: 5.2815 - masked_accuracy: 0.0992\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 8s 303ms/step - loss: 5.1679 - masked_accuracy: 0.1042\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 8s 305ms/step - loss: 5.0555 - masked_accuracy: 0.1139\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 8s 310ms/step - loss: 4.9309 - masked_accuracy: 0.1335\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 8s 308ms/step - loss: 4.7949 - masked_accuracy: 0.1556\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 8s 311ms/step - loss: 4.6801 - masked_accuracy: 0.1646\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 7s 297ms/step - loss: 4.5748 - masked_accuracy: 0.1738\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 7s 300ms/step - loss: 4.4806 - masked_accuracy: 0.1809\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 7s 289ms/step - loss: 4.3952 - masked_accuracy: 0.1876\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 7s 295ms/step - loss: 4.3269 - masked_accuracy: 0.1911\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 7s 287ms/step - loss: 4.2520 - masked_accuracy: 0.1978\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 7s 294ms/step - loss: 4.1850 - masked_accuracy: 0.2020\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 8s 309ms/step - loss: 4.1278 - masked_accuracy: 0.2087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7053fdd030>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJqzPiPoGOCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LL58IFvsGN2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqBo5_GoGNmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}